---
abstract: We introduce a novel policy learning method that integrates analytical gradients from differentiable environments with the Proximal Policy Optimization (PPO) algorithm. To incorporate analytical gradients into the PPO framework, we introduce the concept of an α-policy that stands as a locally superior policy. By adaptively modifying the α value, we can effectively manage the influence of analytical policy gradients during learning. To this end, we suggest metrics for assessing the variance and bias of analytical gradients, reducing dependence on these gradients when high variance or bias is detected. Our proposed approach outperforms baseline algorithms in various scenarios, such as function optimization, physics simulations, and traffic control environments. Our code can be found
online: https://github.com/SonSang/gippo.
slides: ""
url_pdf: "https://openreview.net/pdf?id=oFaLc6fHSt"
publication_types:
  - "1"
authors:
  - Sanghyun Son
  - Laura Yu Zheng
  - Ryan Sullivan
  - Yi-Ling Qiao
  - Ming C. Lin
author_notes: []
publication: NeurIPS 2023
summary: ""
url_dataset: ""
url_project: ""
publication_short: ""
url_source: ""
url_video: ""
title: Gradient Informed Proximal Policy Optimization
doi: ""
featured: false
tags: []
projects: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
date: 2023-12-01T00:00:00Z
url_slides: ""
publishDate: 2023-12-01T00:00:00Z
url_poster: ""
url_code: "https://github.com/SonSang/gippo"
---

{{% callout note %}}
Click the *Cite* button above to demo the feature to enable visitors to import publication metadata into their reference management software.
{{% /callout %}}

{{% callout note %}}
Create your slides in Markdown - click the *Slides* button to check out the example.
{{% /callout %}}

Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/).
